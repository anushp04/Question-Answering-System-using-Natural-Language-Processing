# Question-Answering-model-using-BERT-and-its-derivatives

Question Answering Systems allow users to express a question in natural language and get an immediate and brief response. By leveraging a knowledge base as background, Question Answering (QA) models are used to automate the response to frequently requested questions.
Question Answering models are helpful for intelligent virtual assistants, employed in customer support, or for enterprise Frequently Asked Questions bots. Many search engines add rapid replies to their search results, giving users quick access to information pertinent to their inquiries. 
The dataset being used for this project is SQuAD1.1. which is a reading comprehension dataset. It has more than 100,000 question-answer pairs on about 500 Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. 

We plan to build a Question Answering model that will predict answers for associated questions using three different techniques. First, we will create baseline models based on word embedding techniques which is a learned representation for text where words that have the same meaning have a similar representation. After working on baseline models we are planning to implement transformer-based models such as BERT and ALBERT since transformers solve tasks sequence-to-sequence while easily handling long-distance dependencies thus enabling us to  learn context and subsequently meaning by tracking relationships in sequential data like the words in this sentence. If time permits we will also try to i
